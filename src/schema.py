from typing import Literal, Iterable, Optional, Dict, Union, List, Required

import httpx
from openai import NotGiven
from openai.types.chat import completion_create_params, ChatCompletionToolChoiceOptionParam, \
    ChatCompletionToolParam, ChatCompletionSystemMessageParam, ChatCompletionUserMessageParam
from pydantic import BaseModel, Field
from typing_extensions import TypedDict

ProviderType = Literal["openai", "moonshot"]


class ChatCompletionAssistantMessageParam(TypedDict, total=False):
    """
    todo: fastapi with function call
    """
    
    role: Required[Literal["assistant"]]
    """The role of the messages author, in this case `assistant`."""
    
    content: Optional[str]
    """The contents of the assistant message.

    Required unless `tool_calls` or `function_call` is specified.
    """
    
    # function_call: FunctionCall
    """Deprecated and replaced by `tool_calls`.

    The name and arguments of a function that should be called, as generated by the
    model.
    """
    
    name: str
    """An optional name for the participant.

    Provides the model information to differentiate between participants of the same
    role.
    """
    
    # tool_calls: Iterable[ChatCompletionMessageToolCallParam]
    """The tool calls generated by the model, such as function calls."""


Messages = Iterable[Union[
    ChatCompletionSystemMessageParam,
    ChatCompletionUserMessageParam,
    ChatCompletionAssistantMessageParam,
]]


class LLMBodyBase(BaseModel):
    """
    todo: json --> openai MAP
    """
    
    model: str
    messages: Messages
    frequency_penalty: Optional[float] = None
    
    logprobs: Optional[bool] = None
    presence_penalty: Optional[float] = None
    response_format: completion_create_params.ResponseFormat = None
    seed: Optional[int] = None
    stop: Union[Optional[str], List[str]] = None
    stream: Optional[Literal[False]] | Literal[True] = None
    temperature: Optional[float] = None
    tool_choice: ChatCompletionToolChoiceOptionParam = None
    tools: Iterable[ChatCompletionToolParam] = None
    top_logprobs: Optional[int] = None
    top_p: Optional[float] = None
    user: str = None
    
    n: Optional[int] | NotGiven = 1  # 最小要1个，否则会报错
    max_tokens: Optional[int] | NotGiven \
        = Field(4e3, description="回复的最大token长度，不可以设置超过模型的长度，3.5-turbo总共最大4096，程序发送时可以直接忽略该字段")
    timeout: float | httpx.Timeout | None = 5000  # N毫秒内响应，否则会 connection error
    
    logit_bias: Optional[Dict[str, int]] = {}
    
    ## NOTE: functions deprecated by tools
    # function_call: completion_create_params.FunctionCall = None
    # functions: Iterable[completion_create_params.Function] = None
    
    ## NOTE: Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
    ## The extra values given here take precedence over values defined on the client or passed to this method.
    # extra_headers: Headers | None = None
    # extra_query: Query | None = None,
    # extra_body: Body | None = None,
    
    class Config:
        arbitrary_types_allowed = True


OpenAIModel = Literal[
    "gpt-3.5-turbo",
    "gpt-3.5-turbo-16k",
    "gpt-3.5-turbo-0301",
    "gpt-3.5-turbo-0613",
    "gpt-3.5-turbo-1106",
    "gpt-3.5-turbo-0125",
    "gpt-3.5-turbo-16k-0613",
    
    "gpt-4-0125-preview",
    "gpt-4-turbo-preview",
    "gpt-4-1106-preview",
    "gpt-4-vision-preview",
    "gpt-4",
    "gpt-4-0314",
    "gpt-4-0613",
    "gpt-4-32k",
    "gpt-4-32k-0314",
    "gpt-4-32k-0613",
]


class OpenAIBody(LLMBodyBase):
    model: OpenAIModel


MoonshotModel = Literal[
    "moonshot-v1-8k",
    "moonshot-v1-32k",
    "moonshot-v1-128k"
]

ZhipuModel = Literal[
    "glm-4"
]

MinimaxModel = Literal[
    "abab6-chat",
    "abab5.5-chat",
    "abab5.5s-chat"
]

ModelType = Literal[OpenAIModel, MoonshotModel, ZhipuModel, MinimaxModel]


class MoonshotBody(LLMBodyBase):
    model: MoonshotModel
